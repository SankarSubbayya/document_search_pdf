# Visual Comparison of Chunking Strategies

This document provides visual representations of how each chunking strategy processes documents differently.

---

## ğŸ“„ Example Document

Consider this sample document:

```markdown
# Machine Learning Fundamentals

Machine learning is a subset of artificial intelligence that enables 
systems to learn from data without being explicitly programmed.

## Supervised Learning

Supervised learning uses labeled data to train models. The algorithm 
learns from examples with known outcomes and makes predictions on new data.

### Classification

Classification predicts discrete categories. Common algorithms include 
decision trees, random forests, and neural networks.

### Regression

Regression predicts continuous values. Popular methods are linear 
regression, polynomial regression, and ridge regression.

## Unsupervised Learning

Unsupervised learning finds patterns in unlabeled data without predefined 
categories or targets.
```

---

## 1ï¸âƒ£ Traditional Token Chunking (Baseline)

**How it works**: Splits text every N tokens regardless of structure.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Machine learning is a subset of    â”‚ Chunk 1 (arbitrary split)
â”‚ artificial intelligence that       â”‚
â”‚ enables systems to learn from      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ data without being explicitly      â”‚ Chunk 2 (mid-sentence)
â”‚ programmed. Supervised learning    â”‚
â”‚ uses labeled data to train models. â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ The algorithm learns from examples â”‚ Chunk 3 (no context)
â”‚ with known outcomes and makes      â”‚
â”‚ predictions on new data.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Issues**:
- âŒ Splits mid-sentence
- âŒ Loses semantic meaning
- âŒ No structure awareness
- âŒ Context lost at boundaries

---

## 2ï¸âƒ£ Markup Chunking (Structure-Aware)

**How it works**: Chunks based on document structure (headings, sections).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ # Machine Learning Fundamentals            â”‚ Chunk 1
â”‚                                             â”‚ Hierarchy: ["ML Fundamentals"]
â”‚ Machine learning is a subset of artificial â”‚
â”‚ intelligence that enables systems...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ## Supervised Learning                      â”‚ Chunk 2
â”‚                                             â”‚ Hierarchy: ["ML Fundamentals",
â”‚ Supervised learning uses labeled data...   â”‚              "Supervised Learning"]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ### Classification                          â”‚ Chunk 3
â”‚                                             â”‚ Hierarchy: ["ML Fundamentals",
â”‚ Classification predicts discrete...         â”‚              "Supervised Learning",
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              "Classification"]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ### Regression                              â”‚ Chunk 4
â”‚                                             â”‚ Hierarchy: ["ML Fundamentals",
â”‚ Regression predicts continuous...          â”‚              "Supervised Learning",
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              "Regression"]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ## Unsupervised Learning                    â”‚ Chunk 5
â”‚                                             â”‚ Hierarchy: ["ML Fundamentals",
â”‚ Unsupervised learning finds patterns...    â”‚              "Unsupervised Learning"]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits**:
- âœ… Preserves document structure
- âœ… Semantic sections intact
- âœ… Hierarchical context
- âœ… Natural boundaries

---

## 3ï¸âƒ£ Context Chunking (With Surrounding Context)

**How it works**: Adds context from previous and next chunks.

```
                    â”Œâ”€â”€â”€ Context Before â”€â”€â”€â”
                    â”‚                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚ [...artificial intelligence...]  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                                            â”‚  â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“ â”‚  â”‚ Chunk 2
â”‚ â”ƒ ## Supervised Learning                â”ƒ â”‚  â”‚ (Main Content)
â”‚ â”ƒ                                        â”ƒ â”‚  â”‚
â”‚ â”ƒ Supervised learning uses labeled      â”ƒ â”‚  â”‚
â”‚ â”ƒ data to train models. The algorithm   â”ƒ â”‚  â”‚
â”‚ â”ƒ learns from examples with known       â”ƒ â”‚  â”‚
â”‚ â”ƒ outcomes and makes predictions...     â”ƒ â”‚  â”‚
â”‚ â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”› â”‚  â”‚
â”‚                                            â”‚  â”‚
â”‚ [...Classification predicts...]  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â””â”€â”€â”€ Context After â”€â”€â”€


When retrieving this chunk, you get:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context Before: "...artificial             â”‚
â”‚   intelligence that enables systems..."    â”‚
â”‚                                            â”‚
â”‚ Main Content: "## Supervised Learning      â”‚
â”‚   Supervised learning uses labeled data    â”‚
â”‚   to train models..."                      â”‚
â”‚                                            â”‚
â”‚ Context After: "...Classification predicts â”‚
â”‚   discrete categories..."                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits**:
- âœ… Context across boundaries
- âœ… Better semantic understanding
- âœ… Resolves ambiguous references
- âœ… Improved retrieval accuracy

---

## 4ï¸âƒ£ Late Chunking (Contextual Embeddings)

**How it works**: Embeds full document first, then chunks while preserving context.

```
Step 1: Compute Full Document Embedding
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Full Document Text]                            â”‚
â”‚ # Machine Learning Fundamentals                 â”‚
â”‚ Machine learning is a subset...                 â”‚
â”‚ ## Supervised Learning...                       â”‚
â”‚ ### Classification...                           â”‚
â”‚ ### Regression...                               â”‚
â”‚ ## Unsupervised Learning...                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Document Embedding   â”‚
        â”‚ [0.23, -0.15, 0.67,â”‚
        â”‚  0.82, -0.34, ...]  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Step 2: Chunk Text & Compute Individual Embeddings
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1          â”‚     â”‚ Chunk Embedding  â”‚
â”‚ "ML is a subset" â”‚â”€â”€â”€â”€â–¶â”‚ [0.45, -0.12,...]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Step 3: Create Contextual Embedding (Blend)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Contextual Embedding = 70% Chunk + 30% Documentâ”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Chunk Emb   â”‚      â”‚ Document Emb â”‚        â”‚
â”‚  â”‚ [0.45,...]  â”‚ 70%  â”‚ [0.23,...]   â”‚ 30%   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                    â”‚                 â”‚
â”‚         â–¼                    â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   Blended Contextual Embedding   â”‚         â”‚
â”‚  â”‚   [0.38, -0.13, 0.48, ...]       â”‚         â”‚
â”‚  â”‚                                   â”‚         â”‚
â”‚  â”‚ âœ“ Understands chunk content       â”‚         â”‚
â”‚  â”‚ âœ“ Preserves document context      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Result for each chunk:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk Text: "Supervised learning..."  â”‚
â”‚                                        â”‚
â”‚ Chunk Embedding: [0.45, -0.12, ...]   â”‚
â”‚ (understands local content)            â”‚
â”‚                                        â”‚
â”‚ Contextual Embedding: [0.38, -0.13,...]â”‚
â”‚ (understands local + global context)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits**:
- âœ… Best retrieval accuracy
- âœ… Global document context preserved
- âœ… Semantic coherence
- âœ… Handles specialized terminology better

---

## ğŸ“Š Side-by-Side Comparison

### Query: "What is supervised learning?"

**Token Chunking**:
```
Retrieved: "data to train models. The algorithm learns from..."
âŒ Missing context about what supervised learning IS
âŒ Starts mid-concept
```

**Markup Chunking**:
```
Retrieved: 
"## Supervised Learning

Supervised learning uses labeled data to train models. 
The algorithm learns from examples with known outcomes..."

âœ… Complete section with heading
âœ… Clear context
âœ… Natural boundaries
```

**Context Chunking**:
```
Retrieved:
Context Before: "...ML is a subset of AI..."

Main: "## Supervised Learning
Supervised learning uses labeled data..."

Context After: "...Classification predicts discrete categories..."

âœ… Understanding of where this fits in the document
âœ… Connection to broader concepts
âœ… Preview of what comes next
```

**Late Chunking**:
```
Retrieved: "Supervised learning uses labeled data..."

Embedding includes context about:
- Machine Learning being a subset of AI âœ“
- Relationship to Classification & Regression âœ“  
- Contrast with Unsupervised Learning âœ“
- Overall document theme (ML fundamentals) âœ“

âœ… Most semantically accurate retrieval
âœ… Understands relationships
âœ… Best match for intent
```

---

## ğŸ¯ Choosing the Right Strategy

### For Technical Documentation
```
Document Structure:
# API Reference
  ## Authentication
    ### OAuth2
    ### API Keys
  ## Endpoints
    ### GET /users
    ### POST /users

âœ… Use MARKUP CHUNKING
   - Preserves API structure
   - Each endpoint is a complete chunk
   - Hierarchy shows relationships
```

### For Long Narratives
```
Document Type: Research Paper / Blog Post

"...Previous work in NLP focused on RNNs. However, 
these models had limitations with long sequences. 
The transformer architecture addressed this by..."

âœ… Use CONTEXT CHUNKING
   - Connects ideas across chunks
   - Understands "these models" reference
   - Preserves narrative flow
```

### For Critical Accuracy
```
Use Case: Medical/Legal Documents
Requirement: Highest possible retrieval accuracy

"The dosage of 50mg should be administered twice 
daily. Contraindications include..."

âœ… Use LATE CHUNKING
   - Most accurate embeddings
   - Understands full medical context
   - Critical for safety
```

---

## ğŸ’¡ Hybrid Approaches

You can combine strategies for best results:

### Approach 1: Markup + Context
```
Step 1: Use Markup to identify sections
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ # Introduction  â”‚ Section 1
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ## Background   â”‚ Section 2
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Add context between sections
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Section 2: "## Background"          â”‚
â”‚ Context from Section 1: "...intro..."â”‚
â”‚ Context from Section 3: "...methods"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Approach 2: Markup + Late
```
Step 1: Use Markup for large sections
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ## Supervised Learning     â”‚ Large section
â”‚ [2000 characters]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Apply Late Chunking to large sections
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sub-chunk 1  â”‚  â”‚ Sub-chunk 2  â”‚  â”‚ Sub-chunk 3  â”‚
â”‚ w/ contextualâ”‚  â”‚ w/ contextualâ”‚  â”‚ w/ contextualâ”‚
â”‚ embeddings   â”‚  â”‚ embeddings   â”‚  â”‚ embeddings   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: Structure preservation + Best embeddings
```

---

## ğŸ“ˆ Performance Visualization

```
Retrieval Accuracy â–²
                  â”‚
            100%  â”‚                          â—  Late
                  â”‚                     â—  
             90%  â”‚                â—    Context
                  â”‚           â—   
             80%  â”‚      â—    Markup
                  â”‚  â—
             70%  â”‚  Semantic
                  â”‚â—
             60%  â”‚ Token
                  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
                   Speed (Faster â”€â”€â”€â”€â”€â”€â–¶ Slower)


Memory Usage     â–²
                  â”‚
            High  â”‚                          â—  Late
                  â”‚
          Medium  â”‚                â—  Context
                  â”‚           â—  
             Low  â”‚      â—    Semantic/Markup
                  â”‚  â—
        Very Low  â”‚  Token
                  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
                   Complexity (Simple â”€â”€â–¶ Complex)
```

---

## ğŸ” Real Example Output

Given this query: **"How does supervised learning differ from unsupervised learning?"**

### Token Chunking Result
```
Score: 0.72
Chunk: "learning uses labeled data to train models. The 
algorithm learns from examples with known outcomes and makes"

âš ï¸ Missing: What IS supervised learning
âš ï¸ Missing: Comparison to unsupervised
```

### Markup Chunking Result
```
Score: 0.81
Chunk: "## Supervised Learning

Supervised learning uses labeled data to train models. The 
algorithm learns from examples with known outcomes..."

Section Path: "ML Fundamentals > Supervised Learning"

âœ“ Complete section
âœ“ Clear hierarchy
âŒ Missing: Direct comparison
```

### Context Chunking Result  
```
Score: 0.87
Main: "## Supervised Learning
Supervised learning uses labeled data..."

Context After: "## Unsupervised Learning
Unsupervised learning finds patterns in unlabeled data..."

âœ“ Has both concepts
âœ“ Natural comparison possible
âœ“ Connected information
```

### Late Chunking Result
```
Score: 0.94
Chunk: "## Supervised Learning
Supervised learning uses labeled data to train models..."

Contextual Embedding captures:
- Relationship to unsupervised learning âœ“
- Position in ML fundamentals âœ“
- Contrast between labeled/unlabeled âœ“
- Overall document theme âœ“

âœ“ Highest score
âœ“ Best semantic match
âœ“ Most relevant to query intent
```

---

## ğŸ“ Summary

| Strategy | Visual Metaphor | Best For |
|----------|----------------|----------|
| **Token** | Cutting paper with ruler | Quick prototyping |
| **Semantic** | Cutting along dotted lines | General purpose |
| **Markup** | Cutting along perforations | Structured docs |
| **Context** | Cutting with transparent tape | Long narratives |
| **Late** | 3D holographic cutting | Maximum accuracy |

---

## ğŸ“š Try It Yourself

Run the demo to see these strategies in action:

```bash
python examples/chunking_strategies_demo.py
```

The demo will show you:
1. How each strategy processes the same document differently
2. What metadata each strategy provides
3. Comparative statistics
4. Real chunk examples

---

**Ready to implement? See:**
- Quick Start: `docs/CHUNKING_QUICK_REFERENCE.md`
- Full Guide: `docs/CHUNKING_STRATEGIES_GUIDE.md`
- Examples: `examples/chunking_strategies_demo.py`

